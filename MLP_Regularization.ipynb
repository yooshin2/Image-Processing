{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (comet2)",
      "language": "python",
      "name": "comet2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yooshin2/Image-Processing/blob/main/MLP_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEZAA1enm9xZ"
      },
      "source": [
        "[제가 미리 만들어놓은 이 링크](https://colab.research.google.com/github/heartcored98/Standalone-DeepLearning/blob/master/Lec4/Lab5_regularization_implemented.ipynb)를 통해 Colab에서 바로 작업하실 수 있습니다!  \n",
        "런타임 유형은 python3, GPU 가속 확인하기!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymEzRdg2qH26",
        "outputId": "ba006d67-d86f-4f5a-e38c-095d911b8a3f"
      },
      "source": [
        "!pip install -q torch==1.7.0 torchvision\r\n",
        "import torch\r\n",
        "print(torch.__version__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD4cIKKvKFCC"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UainWPco7Y"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu753dPPKGkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5639fa10-747b-4b3f-e648-600b3ecbe247"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxnfFJwBcsAv"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G6bZbbkMWWt"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout = dropout\n",
        "        self.use_bn = use_bn\n",
        "        self.use_xavier = use_xavier\n",
        "        \n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for i in range(self.n_layer-1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "            if self.use_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
        "                \n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
        "        \n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act == nn.Tanh()\n",
        "        elif self.act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "        \n",
        "        # ====== Create Regularization Layer ======= #\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        if self.use_xavier:\n",
        "            self.xavier_init()\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        for i in range(len(self.linears)):\n",
        "            x = self.act(self.linears[i](x))\n",
        "            x = self.bns[i](x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def xavier_init(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.xavier_normal_(linear.weight)\n",
        "            linear.bias.data.fill_(0.01)\n",
        "            \n",
        "net = MLP(3072, 10, 100, 4, 'relu', 0.1, True, True) # Testing Model Construction"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itGsp6jDWs_a"
      },
      "source": [
        "## Train, Validate, Test and Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhpr1ANlm9xk"
      },
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True,\n",
        "                                              num_workers=2)\n",
        "    net.train()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.view(-1, 3072)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN50w-Ofm9xl"
      },
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False,\n",
        "                                            num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2ghSLe4m9xm"
      },
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False,\n",
        "                                             num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiOCP6TqWw2V"
      },
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, partition, args)    \n",
        "    return train_loss, val_loss, train_acc, val_acc, test_acc"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omgExzmQgU1J"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRoOy_B3Wu7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e7bfb99-5102-4332-cd15-9a05d81a8c88"
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.in_dim = 3072\n",
        "args.out_dim = 10\n",
        "args.hid_dim = 100\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.dropout = 0.2\n",
        "args.use_bn = True\n",
        "args.l2 = 0.00001\n",
        "args.use_xavier = True\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015\n",
        "args.epoch = 10\n",
        "\n",
        "args.train_batch_size = 256\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'n_layer'\n",
        "name_var2 = 'hid_dim'\n",
        "list_var1 = [3, 3, 4]\n",
        "list_var2 = [500, 300, 700]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "        result = experiment(partition, args)  \n",
        "        print('Test Acc:{}'.format(result[4]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 35.35/38.70, Loss(train/val) 1.84/1.71. Took 12.55 sec\n",
            "Epoch 1, Acc(train/val): 44.58/39.33, Loss(train/val) 1.54/1.76. Took 12.75 sec\n",
            "Epoch 2, Acc(train/val): 49.02/43.71, Loss(train/val) 1.42/1.65. Took 12.89 sec\n",
            "Epoch 3, Acc(train/val): 51.87/43.47, Loss(train/val) 1.34/1.82. Took 12.56 sec\n",
            "Epoch 4, Acc(train/val): 54.61/43.65, Loss(train/val) 1.27/1.65. Took 12.74 sec\n",
            "Epoch 5, Acc(train/val): 56.79/51.10, Loss(train/val) 1.21/1.41. Took 12.40 sec\n",
            "Epoch 6, Acc(train/val): 59.23/50.23, Loss(train/val) 1.14/1.45. Took 12.94 sec\n",
            "Epoch 7, Acc(train/val): 61.36/46.89, Loss(train/val) 1.08/1.56. Took 12.45 sec\n",
            "Epoch 8, Acc(train/val): 63.47/43.95, Loss(train/val) 1.01/2.10. Took 12.51 sec\n",
            "Epoch 9, Acc(train/val): 65.37/50.69, Loss(train/val) 0.96/1.58. Took 12.51 sec\n",
            "Test Acc:50.08\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 36.13/40.36, Loss(train/val) 1.79/1.67. Took 12.62 sec\n",
            "Epoch 1, Acc(train/val): 45.52/38.09, Loss(train/val) 1.52/1.82. Took 12.48 sec\n",
            "Epoch 2, Acc(train/val): 49.16/47.22, Loss(train/val) 1.42/1.49. Took 12.46 sec\n",
            "Epoch 3, Acc(train/val): 51.75/46.76, Loss(train/val) 1.34/1.51. Took 12.90 sec\n",
            "Epoch 4, Acc(train/val): 54.38/49.84, Loss(train/val) 1.27/1.43. Took 12.64 sec\n",
            "Epoch 5, Acc(train/val): 56.69/50.05, Loss(train/val) 1.21/1.45. Took 12.61 sec\n",
            "Epoch 6, Acc(train/val): 58.44/46.68, Loss(train/val) 1.16/1.56. Took 12.37 sec\n",
            "Epoch 7, Acc(train/val): 60.57/50.47, Loss(train/val) 1.10/1.44. Took 12.81 sec\n",
            "Epoch 8, Acc(train/val): 62.27/48.97, Loss(train/val) 1.05/1.45. Took 12.62 sec\n",
            "Epoch 9, Acc(train/val): 64.20/50.90, Loss(train/val) 1.00/1.46. Took 12.76 sec\n",
            "Test Acc:50.63\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.97/33.73, Loss(train/val) 1.91/2.09. Took 12.58 sec\n",
            "Epoch 1, Acc(train/val): 44.35/39.28, Loss(train/val) 1.54/1.92. Took 12.85 sec\n",
            "Epoch 2, Acc(train/val): 49.33/44.01, Loss(train/val) 1.42/1.62. Took 12.67 sec\n",
            "Epoch 3, Acc(train/val): 52.24/44.60, Loss(train/val) 1.33/1.58. Took 12.64 sec\n",
            "Epoch 4, Acc(train/val): 54.77/45.75, Loss(train/val) 1.25/1.62. Took 13.33 sec\n",
            "Epoch 5, Acc(train/val): 57.55/47.75, Loss(train/val) 1.18/1.56. Took 12.50 sec\n",
            "Epoch 6, Acc(train/val): 59.78/44.95, Loss(train/val) 1.12/1.67. Took 12.90 sec\n",
            "Epoch 7, Acc(train/val): 62.16/47.57, Loss(train/val) 1.06/1.70. Took 12.67 sec\n",
            "Epoch 8, Acc(train/val): 63.93/50.06, Loss(train/val) 1.00/1.54. Took 12.39 sec\n",
            "Epoch 9, Acc(train/val): 66.26/54.05, Loss(train/val) 0.94/1.41. Took 12.33 sec\n",
            "Test Acc:52.9\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.97/39.34, Loss(train/val) 1.85/1.70. Took 12.56 sec\n",
            "Epoch 1, Acc(train/val): 44.78/42.54, Loss(train/val) 1.53/1.62. Took 12.35 sec\n",
            "Epoch 2, Acc(train/val): 48.73/46.54, Loss(train/val) 1.42/1.49. Took 12.52 sec\n",
            "Epoch 3, Acc(train/val): 52.26/44.28, Loss(train/val) 1.33/1.63. Took 12.59 sec\n",
            "Epoch 4, Acc(train/val): 54.85/48.12, Loss(train/val) 1.26/1.49. Took 12.69 sec\n",
            "Epoch 5, Acc(train/val): 56.42/47.03, Loss(train/val) 1.21/1.57. Took 13.27 sec\n",
            "Epoch 6, Acc(train/val): 58.81/46.40, Loss(train/val) 1.15/1.66. Took 12.43 sec\n",
            "Epoch 7, Acc(train/val): 61.74/48.49, Loss(train/val) 1.07/1.56. Took 12.77 sec\n",
            "Epoch 8, Acc(train/val): 62.75/49.36, Loss(train/val) 1.03/1.54. Took 12.58 sec\n",
            "Epoch 9, Acc(train/val): 64.56/49.06, Loss(train/val) 0.98/1.59. Took 12.70 sec\n",
            "Test Acc:48.14\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 36.53/40.61, Loss(train/val) 1.77/1.65. Took 12.39 sec\n",
            "Epoch 1, Acc(train/val): 45.61/46.07, Loss(train/val) 1.52/1.52. Took 12.93 sec\n",
            "Epoch 2, Acc(train/val): 49.76/46.64, Loss(train/val) 1.40/1.51. Took 12.58 sec\n",
            "Epoch 3, Acc(train/val): 52.58/48.56, Loss(train/val) 1.33/1.44. Took 12.58 sec\n",
            "Epoch 4, Acc(train/val): 55.16/46.56, Loss(train/val) 1.26/1.54. Took 12.39 sec\n",
            "Epoch 5, Acc(train/val): 57.04/49.93, Loss(train/val) 1.20/1.45. Took 12.39 sec\n",
            "Epoch 6, Acc(train/val): 58.98/50.97, Loss(train/val) 1.14/1.41. Took 12.74 sec\n",
            "Epoch 7, Acc(train/val): 61.30/46.18, Loss(train/val) 1.08/1.58. Took 12.49 sec\n",
            "Epoch 8, Acc(train/val): 62.28/51.74, Loss(train/val) 1.05/1.42. Took 12.69 sec\n",
            "Epoch 9, Acc(train/val): 64.47/49.35, Loss(train/val) 0.99/1.52. Took 12.62 sec\n",
            "Test Acc:49.78\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.12/34.32, Loss(train/val) 1.92/1.94. Took 12.50 sec\n",
            "Epoch 1, Acc(train/val): 43.79/41.68, Loss(train/val) 1.56/1.71. Took 12.83 sec\n",
            "Epoch 2, Acc(train/val): 48.42/43.15, Loss(train/val) 1.44/1.61. Took 12.53 sec\n",
            "Epoch 3, Acc(train/val): 51.55/44.97, Loss(train/val) 1.35/1.61. Took 12.51 sec\n",
            "Epoch 4, Acc(train/val): 54.04/45.20, Loss(train/val) 1.27/1.59. Took 13.05 sec\n",
            "Epoch 5, Acc(train/val): 56.61/51.63, Loss(train/val) 1.21/1.41. Took 12.36 sec\n",
            "Epoch 6, Acc(train/val): 59.12/45.64, Loss(train/val) 1.13/1.72. Took 12.97 sec\n",
            "Epoch 7, Acc(train/val): 61.28/49.73, Loss(train/val) 1.07/1.51. Took 12.63 sec\n",
            "Epoch 8, Acc(train/val): 63.37/50.01, Loss(train/val) 1.02/1.47. Took 12.48 sec\n",
            "Epoch 9, Acc(train/val): 65.26/50.09, Loss(train/val) 0.96/1.51. Took 13.09 sec\n",
            "Test Acc:49.17\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.30/34.67, Loss(train/val) 1.86/1.77. Took 12.93 sec\n",
            "Epoch 1, Acc(train/val): 44.05/38.22, Loss(train/val) 1.56/1.83. Took 13.08 sec\n",
            "Epoch 2, Acc(train/val): 47.67/43.97, Loss(train/val) 1.45/1.58. Took 13.11 sec\n",
            "Epoch 3, Acc(train/val): 51.44/42.48, Loss(train/val) 1.35/1.65. Took 13.02 sec\n",
            "Epoch 4, Acc(train/val): 53.97/44.32, Loss(train/val) 1.29/1.62. Took 12.84 sec\n",
            "Epoch 5, Acc(train/val): 56.18/47.82, Loss(train/val) 1.22/1.48. Took 13.05 sec\n",
            "Epoch 6, Acc(train/val): 58.10/50.55, Loss(train/val) 1.17/1.42. Took 12.82 sec\n",
            "Epoch 7, Acc(train/val): 59.76/48.13, Loss(train/val) 1.11/1.52. Took 13.05 sec\n",
            "Epoch 8, Acc(train/val): 62.40/47.51, Loss(train/val) 1.04/1.66. Took 12.90 sec\n",
            "Epoch 9, Acc(train/val): 64.58/49.80, Loss(train/val) 0.99/1.51. Took 12.79 sec\n",
            "Test Acc:49.86\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 35.75/39.48, Loss(train/val) 1.79/1.69. Took 12.82 sec\n",
            "Epoch 1, Acc(train/val): 44.63/44.96, Loss(train/val) 1.54/1.55. Took 12.84 sec\n",
            "Epoch 2, Acc(train/val): 48.58/44.63, Loss(train/val) 1.44/1.62. Took 12.90 sec\n",
            "Epoch 3, Acc(train/val): 51.60/47.16, Loss(train/val) 1.36/1.51. Took 12.78 sec\n",
            "Epoch 4, Acc(train/val): 53.83/50.75, Loss(train/val) 1.29/1.39. Took 12.67 sec\n",
            "Epoch 5, Acc(train/val): 55.65/50.44, Loss(train/val) 1.23/1.43. Took 12.57 sec\n",
            "Epoch 6, Acc(train/val): 57.67/46.84, Loss(train/val) 1.18/1.56. Took 13.01 sec\n",
            "Epoch 7, Acc(train/val): 59.60/50.43, Loss(train/val) 1.13/1.47. Took 12.46 sec\n",
            "Epoch 8, Acc(train/val): 61.31/47.82, Loss(train/val) 1.08/1.58. Took 12.65 sec\n",
            "Epoch 9, Acc(train/val): 62.53/51.18, Loss(train/val) 1.05/1.41. Took 12.65 sec\n",
            "Test Acc:51.23\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 32.94/36.11, Loss(train/val) 1.96/1.94. Took 12.56 sec\n",
            "Epoch 1, Acc(train/val): 42.91/43.71, Loss(train/val) 1.59/1.65. Took 12.92 sec\n",
            "Epoch 2, Acc(train/val): 47.43/38.36, Loss(train/val) 1.46/1.87. Took 12.91 sec\n",
            "Epoch 3, Acc(train/val): 49.81/39.32, Loss(train/val) 1.40/1.78. Took 12.78 sec\n",
            "Epoch 4, Acc(train/val): 52.85/47.99, Loss(train/val) 1.31/1.47. Took 12.57 sec\n",
            "Epoch 5, Acc(train/val): 55.48/42.62, Loss(train/val) 1.24/1.69. Took 12.74 sec\n",
            "Epoch 6, Acc(train/val): 57.33/44.47, Loss(train/val) 1.19/1.91. Took 12.53 sec\n",
            "Epoch 7, Acc(train/val): 59.77/47.33, Loss(train/val) 1.12/1.59. Took 12.69 sec\n",
            "Epoch 8, Acc(train/val): 62.07/51.30, Loss(train/val) 1.06/1.45. Took 12.98 sec\n",
            "Epoch 9, Acc(train/val): 63.58/47.91, Loss(train/val) 1.02/1.54. Took 13.01 sec\n",
            "Test Acc:48.79\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}